# Efficient Memory Management for Large LanguageModel Serving with PagedAttention

```대형 언어 모델(LLM) 성능 향상을 위한 저메모리 솔루션인 vLLM에 대한 심층적인 설명을 제공합니다. 특히 PagedAttention 알고리즘을 활용하여 메모리 관리의 비효율성을 극복하고 자원을 최적화하며, 처리량을 2-4배 향상시킬 수 있는 방법을 알아볼 수 있습니다. 이를 통해 LLM 서비스의 운영 비용을 줄이고 효율성을 높이는 방법을 배울 수 있으며, 최신 기술적 접근 방식에 대한 통찰을 제공합니다.```

효율적인 메모리 관리는 대형 언어 모델의 성능에 큰 영향을 미친다.
- PagedAttention 알고리즘은 메모리 낭비를 줄이고 처리량을 2-4배 향상시키는 데 기여한다.
- 이 알고리즘은 GPU의 효율성을 극대화하여 요청 간 KV 캐시를 유연하게 공유할 수 있도록 설계되었다.
- 결과적으로, vLLM이라는 시스템은 기존 LLM 서비스의 운영 비용을 줄이고 성능을 개선한다.

대형 언어 모델의 응용 프로그램은 우리의 일상에 큰 영향을 미치고 있다.
- 프로그램 어시스턴트와 보편적 챗봇 같은 새로운 서비스는 LLM의 출현으로 가능해졌다.
- 그러나 이러한 기술을 운영하는 데는 많은 하드웨어 자원과 비용이 소모되기 때문에 최적화가 필요하다.
- PagedAttention과 함께 개발된 vLLM 기술은 이러한 문제를 해결하기 위한 효율적 접근법을 제공한다.

LLM 서비스에 있어서 디지털 메모리의 역할이 점점 더 중요해지고 있다.
- 기존 시스템의 메모리 관리 문제는 내부 단편화와 외부 단편화로 이어져 자원 활용을 떨어뜨린다.
- PagedAttention은 이러한 문제를 해결하기 위해 KV 캐시를 불연속 메모리 공간에 저장할 수 있도록 설계되었다.
- 결과적으로, vLLM은 메모리의 효과적인 할당과 재사용을 통해 성능을 극대화할 수 있다.

vLLM의 디코딩 알고리즘 지원은 다양한 사용 사례를 통해 성능을 향상시킨다.
- vLLM은 다양한 디코딩 방법을 지원하며 복잡한 접근 패턴과 메모리 공유 기회를 제공한다.
- 예를 들어, 병렬 샘플링을 통해 사용자는 여러 후보 출력 중에서 선택할 수 있으며, 메모리를 효율적으로 관리한다.
- 이러한 유연성은 vLLM의 응용 프로그램이 더욱 다양해지도록 하는 데 기여한다.

분산 GPU 환경에서의 실행은 대형 모델 운영에 필수적이다.
- 많은 LLM은 단일 GPU의 용량을 초과하는 크기를 가지기 때문에 분산 처리가 필요하다.
- vLLM은 메모리 관리자가 분산 메모리를 처리할 수 있도록 설계되어, 여러 GPU 사이에서 효과적인 작업을 수행할 수 있다.
- 이 기술은 LLM의 처리 속도와 성능을 크게 향상시키는 데 기여한다.

1. 대형 언어 모델의 효율적인 메모리 관리
PagedAttention 알고리즘은 메모리 관리의 비효율성을 극복하기 위한 도구로 사용된다.
이 알고리즘은 자원을 최적화하여, LLM의 처리량을 2-4배 향상시킬 수 있다.
따라서 LLM 서비스의 운영 비용을 줄이고, 효율성을 높일 수 있다.
연구는 UC Berkeley, Stanford University, 그리고 UC San Diego의 협력으로 진행되었다.
Woosuk Kwon, Zhuohan Li 등의 연구자들이 참여했으며, Ion Stoica 교수도 포함된다.

2. 대규모 언어 모델 효과적 처리 위한 PagedAttention 제안
대규모 언어 모델의 높은 처리량을 위해서는 충분한 요청을 배치해야 하지만, 기존 시스템은 키-값 캐시(KV cache) 메모리가 매우 크고 동적으로 변하기 때문에 어려움을 겪고 있다.
비효율적으로 관리될 경우, 이 메모리는 단편화와 중복으로 인해 상당한 낭비가 발생하여 배치 크기를 제한하게 된다.
이 문제를 해결하기 위해 우리는 운영 체제의 고전적인 가상 메모리와 페이지 기법에서 영감을 받은 PagedAttention이라는 주의 알고리즘을 제안한다.
이 기반 위에 vLLM이라는 LLM 서비스 시스템을 구축하였으며, 이는 (1) KV 캐시 메모리의 낭비를 거의 0으로 줄이고, (2) 요청 간 KV 캐시의 유연한 공유를 통해 메모리 사용을 추가로 줄인다.
평가 결과, vLLM은 FasterTransformer 및 Orca와 같은 최신 시스템과 비교했을 때 동일한 지연 시간에서 인기 있는 LLM의 처리량을 2-4배 향상시킨다.

3. 대형 언어 모델 최적과 메모리 관리.
대형 언어 모델(LLM)의 출현으로 프로그래밍 어시스턴트와 보편적 챗봇 같은 새로운 응용 프로그램이 등장하고 있으며, 이는 우리의 업무와 일상에 큰 영향을 미치고 있다.
하지만 이러한 응용 프로그램을 운영하는 것은 매우 비쌉고, 많은 하드웨어 가속기가 필요하다.
LLM의 기본 모델은 자동 회귀 Transformer 모델로, 입력과 이전의 출력 토큰 기반으로 단어를 생성한다.
이러한 생성 과정은 메모리 사용에 제한을 두고, GPU의 계산 능력을 효과적으로 활용하지 못하게 하며, 사용량을 최적화하려면 요청을 배치하여 처리해야 한다.
이러한 문제를 해결하기 위해, PagedAttention이라는 새로운 주의 메커니즘을 제안하며, 이를 바탕으로 높은 처리량을 달성하는 vLLM라는 기술을 개발하였다.

4. LLM 생성 및 서비스 절차 설명
이 섹션에서는 전형적인 LLM의 생성 및 서비스 절차를 소개한다.
또한 LLM 서비스에 사용되는 반복 수준 스케줄링에 대해 설명한다.

5. ️Transformer 기반 대형 언어 모델의 이해
언어 모델링은 토큰 리스트의 확률을 모델링하는 작업으로, 자연어의 순차적 특성을 반영하여 조건부 확률의 곱으로 전체 확률을 분해하는 방식이 일반적이다.
Transformers는 이러한 확률 모델링에서 표준 아키텍처가 되었으며, 주의 깊은 자기-주목(self-attention) 레이어가 주요 구성 요소이다.
LLM 서비스는 조건부 생성 서비스로 배포되며, 입력 프롬프트를 제공하면 출력 토큰을 생성한다.
요청을 처리할 때 여러 요청을 배치하여 GPU의 효율성을 높일 수 있으나, 요청 시점과 입력 길이가 다르기 때문에 복잡한 기술이 필요하다.
세부 조정된 배치 메커니즘은 대기 지연을 줄이고 패딩으로 인한 비효율성을 제거하여 LLM 서비스를 통한 처리량을 크게 증가시킨다.

6. LLM 서비스의 메모리 관리 과제
세밀한 배치가 컴퓨팅 자원의 낭비를 줄이고 유연한 요청 처리를 가능하게 하지만, 배치할 수 있는 요청 수는 여전히 GPU 메모리 용량에 의해 제약을 받는다.
특히, KV 캐시를 저장하는 공간이 중요한데, 요청 수가 많아짐에 따라 KV 캐시의 크기가 빠르게 증가한다.
예를 들어, 13B 파라미터 OPT 모델의 경우, 단일 토큰의 KV 캐시 크기는 800KB까지 요구되며, OPT는 최대 2048개의 토큰을 생성할 수 있어 요청당 1.6GB의 메모리가 필요하다.
GPU의 메모리 용량이 수십 GB에 달하지만, 모든 메모리를 KV 캐시에 할당해도 단 몇십 개의 요청만 처리할 수 있다.
또한, 메모리 관리를 비효율적으로 할 경우 배치 크기가 더 줄어들 수 있으며, 최근 경향에 따르면 GPU의 연산 속도가 메모리 용량보다 더 빨리 증가하고 있다.

7. 기존 시스템의 메모리 관리
현재의 딥러닝 프레임워크에서는 텐서가 연속 메모리에 저장되어야 하므로, 이전 LLM 서비스 시스템도 요청의 KV 캐시를 연속 텐서로 저장합니다.
LLM의 예측할 수 없는 출력 길이 때문에, 요청의 최대 시퀀스 길이에 기반하여 메모리를 정적으로 할당하게 되며, 실제 입력이나 출력 길이와는 무관하게 진행됩니다.
기존 시스템에서 발생하는 메모리 낭비의 주요 원인은 미래의 토큰을 위한 예약 슬롯, 최대 시퀀스 길이에 대한 과도한 할당으로 인한 내부 단편화, 그리고 메모리 할당자에서 발생하는 외부 단편화입니다.
특히, 예상된 생성 토큰에 대해 외부 단편화는 사용되지 않으며, 예약된 메모리는 요청의 전체 기간 동안 사용되지만 다른 요청을 처리하는 데 필요한 공간을 차지하므로 비효율적입니다.
실험 결과에 따르면, 이전 시스템의 실제 유효 메모리는 최대 20.4%에 불과함을 보여줍니다.

8. 페이지 주의 알고리즘과 vLLM 엔진 개발
이 작업에서는 새로운 주의 알고리즘인 PagedAttention을 개발하고, §3에서 설명한 문제를 해결하기 위한 LLM 서버 엔진 vLLM을 구축한다.
vLLM의 아키텍처는 중앙 집중식 일정 관리자를 사용하여 분산된 GPU 작업자의 실행을 조정한다.
KV 캐시 관리자는 PagedAttention에 의해 제공되는 페이지 형식으로 KV 캐시를 효과적으로 관리하며, 중앙 집중식 일정 관리자에 의해 전송된 지침을 통해 GPU 작업자의 물리적 KV 캐시 메모리를 관리한다.
이어지는 §4.1에서는 PagedAttention 알고리즘을 설명하고, §4.2에서는 KV 캐시 관리자의 설계를, §4.3에서는 PagedAttention을 지원하는 방법을 논의한다.
마지막으로 §4.4에서는 다양한 디코딩 방법을 위한 효과적인 메모리 관리 방법과 §4.5에서는 가변 길이 입력 및 출력 시퀀스를 처리하는 방법을 설명하며, §4.6에서는 분산 환경에서 vLLM의 시스템 설계가 작동하는 방식을 보여준다.

9. PagedAttention: 메모리 문제 해결을 위한 알고리즘
PagedAttention은 운영체제의 페이지 개념에 영감을 받아 메모리 문제를 해결하기 위해 도입된 주목 알고리즘이다.
기존의 주목 알고리즘과 달리, PagedAttention은 연속된 키와 값을 불연속 메모리 공간에 저장할 수 있도록 한다.
이 알고리즘은 각 시퀀스의 KV 캐시를 KV 블록으로 나누며, 각 블록은 고정된 수의 토큰에 대한 키 및 값 벡터를 포함한다.
주목 계산 과정에서 PagedAttention 커널은 서로 다른 KV 블록을 개별적으로 식별하고 가져온다.
이 알고리즘은 KV 블록이 불연속 물리 메모리에 저장될 수 있게 해주므로, vLLM에서 더 유연한 페이지 메모리 관리를 가능하게 한다.

10. ️KV 캐시 관리와 메모리 효율성
vLLM의 KV 캐시 관리자는 가상 메모리 개념을 기반으로 하며, 물리적 페이지와 논리적 페이지 간의 맵핑을 통해 효율성을 도모한다.
이 시스템에서는 물리적 메모리를 미리 예약할 필요 없이, 동적으로 페이지를 할당하여 메모리 낭비를 줄인다.
예를 들어, vLLM은 요청에 대한 논리적 KV 블록을 고정 크기의 KV 블록으로 구성하며, 새로운 토큰 생성 시 이전 블록을 이용한다.
디코딩 과정에서 vLLM은 PagedAttention 알고리즘을 사용하여 각 요청의 KV 캐시를 효율적으로 관리하고, 과거 블록 접근 및 새로운 KV 캐시 저장을 통해 성능을 극대화한다.
결국, 메모리의 자유로운 할당과 재사용을 통해 여러 요청을 동시에 효율적으로 처리할 수 있다.

11. 다양한 디코딩 시나리오에 대한 적용
§4.3에서는 PagedAttention과 vLLM이 기본 디코딩 알고리즘을 처리하는 방법에 대해 설명하며, 복잡한 접근 패턴과 메모리 공유 기회를 제공하는 더 복잡한 디코딩 시나리오에 대해 vLLM의 일반적인 적용성을 보여준다.
병렬 샘플링에서는 LLM이 단일 입력 프로프트에 대해 여러 개의 샘플 출력 결과를 생성하여 사용자가 후보 출력 중에서 좋아하는 것을 선택할 수 있도록 하며, vLLM은 이러한 샘플 간의 KV 캐시를 공유할 수 있도록 구현되어 메모리를 절약한다.
빔 검색에서는 LLM이 사용자가 기대하는 여러 적절한 번역 결과를 출력하며, 후보 블록과 나머지 블록을 공유하는 방식이 사용된다.
공유 접두사는 사용자가 제공한 태스크 설명 부분으로, 많은 사용자 프로프트가 동일한 접두사를 공유하므로 이전 계획에 따라 KV 캐시를 미리 저장하여 중복 계산을 줄일 수 있다.
혼합 디코딩 방법들은 다양한 메모리 공유 및 접근 패턴을 보여주며, vLLM은 서로 다른 디코딩 생성 선호도를 가진 요청을 동시에 처리할 수 있도록 지원한다.

12. 4.5 스케줄링 및 선점
시스템 용량을 초과하는 요청 트래픽이 발생할 경우, vLLM은 요청의 하위 집합을 우선 처리해야 한다.
vLLM에서는 모든 요청에 대해 '선착순 방식'(FCFS) 스케줄링 정책을 채택하여 공정성을 보장하고 starvation을 방지한다.
vLLM이 요청을 선점해야 할 경우, 가장 먼저 도착한 요청을 먼저 처리하고, 가장 최근의 요청을 우선적으로 선점한다.
LLM 서비스는 입력 프롬프트의 길이에 따라 예측할 수 없는 출력 길이 문제가 있어, 요청이 증가하면 vLLM은 GPU의 물리적 블록이 부족해질 수 있다.
vLLM은 두 가지 클래식 질문에 대답해야 하는데, (1) 어떤 블록을 퇴출할 것인가? (2) 필요 시 퇴출된 블록을 어떻게 복구할 것인가?

13. 분산 실행의 필요성 및 vLLM의 효과
많은 LLM들은 단일 GPU의 용량을 초과하는 매개변수 크기를 가지고 있다.
따라서 이를 분산 GPU에 나누어 모델 병렬 방식으로 실행할 필요가 있다.
이러한 실행을 위해 분산 메모리를 처리할 수 있는 메모리 관리자가 필요하다.
vLLM은 Transformers에서 널리 사용되는 Megatron-LM 스타일의 텐서 모델 병렬 처리 전략을 지원하여 분산 환경에서 효과적이다.

14. 모델 크기 및 서버 구성
표 1은 모델 크기(13B, 66B, 175B)와 GPU(A100 4개, 8개-80GB) 구성에 대한 정보를 제공한다.
각 모델은 파라미터 크기와 KV 캐시 메모리를 가지며, 블록 단위로 행렬 곱셈을 수행하고 GPU 간에 중간 결과를 동기화한다.
특히, 주의 연산자는 주의 헤드 차원에 따라 나누어지며, 각 SPMD 프로세스는 멀티 헤드 주의에서 일부 주의 헤드를 관리한다.
모델 병렬 실행을 통해 각 모델 조각은 동일한 입력 토큰 집합을 처리하므로 동일한 위치에 대한 KV 캐시가 필요하다.
vLLM은 중앙 집중식 스케줄러 내에 단일 KV 캐시 관리자를 특징으로 하며, GPU 작업자는 이 관리자를 공유하여 각 입력 요청에 대한 물리적 블록을 실행한다.

15. vLLM 구현 및 기능 개요
vLLM은 FastAPI 기반의 프론트엔드와 GPU 기반의 추론 엔진으로 구성된 엔드 투 엔드 서빙 시스템이다.
프론트엔드는 OpenAI API 인터페이스를 확장하여 사용자가 요청에 대해 최대 시퀀스 길이 및 빔 너비 𝑘와 같은 샘플링 매개변수를 사용자화할 수 있도록 한다.
vLLM 엔진은 8,500줄의 Python과 2,000줄의 C++/CUDA 코드로 작성되었고, 제어 관련 구성 요소인 스케줄러와 블록 매니저를 Python으로 개발한다.
또한, PagedAttention과 같은 주요 운영을 위해 커스텀 CUDA 커널을 개발하며, 모델 실행기에서는 GPT, OPT, LLaMA와 같은 인기 LLM을 구현한다.
분산 GPU 작업자 간의 텐서 통신을 위해 NCCL을 사용한다.

16. ️커널 수준 최적화 방법
PagedAttention은 기존 시스템에서 효율적으로 지원되지 않는 메모리 접근 패턴을 도입하므로, 이를 최적화하기 위한 여러 GPU 커널을 개발했다.
(1) 융합된 재구성 및 블록 쓰기: 모든 Transformer 레이어에서 새로운 KV 캐시는 블록으로 나뉘며, 블록 읽기에 최적화된 메모리 레이아웃으로 재구성된 후 블록 테이블에 지정된 위치에 저장된다.
커널 실행 오버헤드를 최소화하기 위해 이를 단일 커널로 융합한다.
(2) 블록 읽기와 어텐션 통합: FasterTransformer의 어텐션 커널을 수정하여 블록 테이블에 따라 KV 캐시를 읽고 실시간으로 어텐션 작업을 수행한다.
메모리 접근이 통합되도록 GPU 와프를 각 블록을 읽도록 지정하며, 요청 배치 내에서 변수 길이 시퀀스 지원을 추가한다.
(3) 융합된 블록 복사: 복사-위치 변경 메커니즘에 의해 발생하는 블록 복사 작업은 불연속 블록에서 동작할 수 있지만, cudaMemcpyAsync API를 사용할 경우 작은 데이터 이동이 빈번하게 발생할 수 있다.
이를 완화하기 위해 서로 다른 블록의 복사 작업을 단일 커널 실행으로 배치하는 커널을 구현하였다.

17. 다양한 디코딩 알고리즘 지원
vLLM은 fork, append, 그리고 free라는 세 가지 주요 방법을 사용하여 다양한 디코딩 알고리즘을 구현한다.
fork 방법은 기존 시퀀스에서 새로운 시퀀스를 생성하며, append 방법은 시퀀스에 새로운 토큰을 추가한다.
마지막으로 free 방법은 시퀀스를 삭제한다.
예를 들어, 병렬 샘플링에서 vLLM은 fork 방법을 사용하여 단일 입력 시퀀스에서 여러 출력 시퀀스를 생성한 후, 매 반복마다 append로 새로운 토큰을 추가하고, 중지 조건을 충족하는 시퀀스를 free로 삭제한다.
vLLM은 이러한 전략을 빔 서치와 접두사 공유에도 적용하며, 미래의 디코딩 알고리즘도 이러한 방법들의 결합으로 지원될 것으로 믿는다.

18. ️📊 다양한 작업 부하에서 vLLM 성능 평가
본 섹션에서는 다양한 작업 부하에서 vLLM의 성능을 평가한다.
OPT 모델과 함께 ShareGPT 및 Alpaca 데이터셋을 사용하여 실제 LLM 서비스의 입력 및 출력 텍스트를 기반으로 작업 부하를 생성하였다.
각 모델의 throughput 및 지연 시간을 측정하며, vLLM은 Orca 및 FasterTransformer에 비해 더 높은 요청 속도를 유지할 수 있었다.
또한, vLLM은 메모리 공유를 통해 성능 개선을 이루었으며, prefix를 공유하는 경우 더 높은 throughput을 기록할 수 있었다.
결과적으로 vLLM은 기본 샘플링 및 빔 검색 방법을 통해 Orca보다 평균적으로 1.3배에서 3.58배 더 높은 throughput을 제공하였다.

19. 챗봇 구현 및 성능 평가
챗봇은 LLM의 가장 중요한 응용 프로그램 중 하나로, 모델이 채팅 기록과 사용자 쿼리를 결합하여 응답을 생성하도록 한다.
ShareGPT 데이터셋을 사용하여 채팅 기록과 사용자 쿼리를 합성하고, OPT-13B 모델의 제한된 컨텍스트 길이로 인해 마지막 1024 토큰까지 잘라내어 최대 1024 토큰을 생성한다.
다른 대화 라운드 간 KV 캐시를 저장하지 않으며, 이는 대화 간 요청을 위한 공간을 차지하게 될 수 있다.
vLLM은 세 가지 Orca 기준과 비교하여 요청 비율을 2배 더 처리할 수 있고, ShareGPT 데이터셋에 긴 대화가 많아 대부분의 요청에 대해 1024 토큰의 입력 프롬프트가 생성된다.
PagedAttention은 메모리 파편화 문제를 해결하여 긴 프롬프트 처리를 효과적으로 지원한다.

20. vLLM의 설계 선택과 성능 연구
이 섹션에서는 vLLM의 여러 측면과 설계 선택을 절단 실험을 통해 연구합니다.
PagedAttention에서의 동적 블록 매핑은 저장된 KV 캐시와 관련된 GPU 작업의 성능에 영향을 미치며, 기존 시스템보다 주의 커널 지연 시간이 20-26% 더 높습니다.
블록 크기의 선택이 vLLM의 성능에 상당한 영향을 미치며, 블록 크기가 작으면 GPU의 병렬 처리 능력을 충분히 활용하지 못하고, 블록 크기가 크면 내부 단편화가 증가합니다.
우리는 재계산과 스와핑 두 가지 회복 메커니즘의 성능을 비교하여, 작은 블록 크기에서는 스와핑이 과도한 오버헤드를 발생시키고, 중간 블록 크기에서는 두 방법의 성능이 비슷하다는 것을 확인했습니다.
vLLM의 가상 메모리 및 페이징 기술은 LLM 서빙에 효율적이지만, 모든 GPU 작업에 일반적으로 적용되지는 않을 수 있습니다.

21. 모델 서빙 시스템 및 메모리 최적화 기법
최근 몇 년간 모델 서빙은 적극적인 연구 분야로 자리 잡았으며, Clipper, TensorFlow Serving, Nexus, InferLine, Clockwork와 같은 여러 시스템이 제안되어 다루어진다.
이 시스템들은 배치 처리, 캐싱, 배치, 스케줄링을 연구하여 단일 또는 다수 모델 서빙의 다양한 측면을 다룬다.
특히 변환기 아키텍처의 중요성을 바탕으로, GPU 커널 최적화, 고급 배치 메커니즘, 모델 병렬성 및 매개변수 공유를 활용한 여러 전문화된 서빙 시스템이 개발되었다.
Orca와 vLLM의 비교에서, Orca의 반복 수준 스케줄링과 vLLM의 페이지 주의는 보완적인 기법이지만, vLLM은 메모리 활용도를 높여 추가 요청을 병렬로 처리하며 속도 개선을 이룬다.
마지막으로, 메모리 최적화 기술을 통해 메모리가 훈련 및 추론에서 병목 현상으로 작용하는 것을 해결하기 위한 새로운 블록 수준 메모리 관리 아이디어도 소개하고 있다.

22. 📄 PagedAttention을 통한 메모리 효율적 LLM 구현
이 논문에서는 PagedAttention이라는 새로운 주의 알고리즘을 제안하고, 비연속적인 페이지 메모리에 키와 값을 저장하여 vLLM이라는 고처리량 LLM 서비스 시스템을 소개했다.
운영 체제에서 영감을 받아, 가상 메모리와 복사-온-쓰기 같은 기존 기술을 활용하여 KV 캐시를 효율적으로 관리하고 다양한 디코딩 알고리즘을 처리할 수 있도록 했다.
실험 결과, vLLM은 최첨단 시스템에 비해 2-4배의 처리량 향상을 이루었다.
이 연구는 여러 기관의 후원으로 진행되었으며, 피드백을 제공한 많은 사람들에게 감사의 뜻을 전한다.

23. 주요 연구자료 및 기술 문헌 목록
2022년 Neural Information Processing Systems 35에서 io-awareness에 대한 연구가 발표되었다.
TurboTransformers는 2021년 ACM SIGPLAN Symposium에서 GPU 기반 변환기 모델의 효율적인 서빙 시스템에 대해 논의하였다.
FastAPI는 2023년에 GitHub에서 제공되며, 저지연 RNN 추론 관련 연구가 2018년에 EuroSys Conference에서 발표되었다.
AI와 메모리 벽에 관한 논문이 2021년에 RiseLab Medium에서 다루어졌고, 다양한 기술적 논문들이 DNN 성능 및 최적화와 관련된 최신 연구를 제공합니다.
NVIDIA의 Triton Inference Server와 FasterTransformer는 최신 AI 추론 시스템에 중요한 역할을 한다.

24. 관련 자료 및 출처
NVIDIA의 NCCL: NVIDIA 집합 통신 라이브러리에 대한 자료는 2023년에 제공되며, 자세한 내용은 https://developer.nvidia.com/nccl에서 확인할 수 있다.
또한, Christopher Olston 외 7명은 2017년에 'Tensorflow-serving: Flexible, high-performance ml serving'에 관한 연구를 arXiv에 발표하였다.
OpenAI와 관련된 자료는 2020년에 제공된 API 소개, 2022년의 ChatGPT에 대한 블로그, 그리고 2023년의 맞춤형 지침에 관한 블로그가 있다.

25. ️📚 GPT-4 및 관련 연구 문헌
OpenAI의 GPT-4 기술 보고서와 LMSYS ORG의 Chatbot Arena 리더보드 관련 자료가 있다.
또한 PyTorch를 포함한 여러 연구들이 신경망 훈련 및 중요한 기술적 정보를 다루고 있다.
특히 POET는 작은 장치에서의 신경망 훈련에 관한 연구로 주목받고 있으며, Transformer 추론의 효율성을 개선하는 연구도 진행되고 있다.

26. 연구 및 기술 문헌 목록
Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He가 2021년에 발표한 'ZeRO-Offload: 민주화하는 거대 모델 훈련'과 관련된 여러 기술 문헌이 있다.
Reuters는 2023년에 AI 기술 거대 기업들이 직면한 문제에 대한 기사를 게재하였으며, Amazon Web Services는 Bedrock 서비스를 제공하고 있다.
또한 Haichen Shen 등은 DNN 기반 비디오 분석을 가속화하기 위한 GPU 클러스터 엔진 'Nexus'를 개발하였다.
Ying Sheng 등은 단일 GPU로 대규모 언어 모델의 고속 생성 추론을 제안하였고, Ilya Sutskever와 팀은 신경망을 사용한 시퀀스 학습을 연구하였다.
마지막으로, Thomas Wolf는 자연어 처리를 위한 최첨단 기술을 연구하고 있으며, Google의 신경 기계 번역 시스템은 인간과 기계 번역 간의 격차를 해소하고 있다.

27. 최근의 연구 업적들 및 컨퍼런스 발표
Byung-Gon Chun의 연구는 2022년 'Orca: A Distributed Serving System for Transformer-Based Generative Models'라는 제목으로 '16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)'에서 발표되었다.
또한, 2023년에는 홍 장 등 여러 연구자들이 'SHEPHERD: Serving DNNs in the Wild'을 주제로 '20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)'에서 발표하였다.
2022년, 수잔 장 외 연구자들은 'Opt: Open pre-trained transformer language models'를 발표하였고, 'arXiv'에 사전 인쇄본으로 등록되었다.
리안민 정 등은 'Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning'을 발표했으며, '16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)'에서 다루어졌다.
마지막으로, 저우 제 등은 2022년에 'PetS: A Unified Framework for Parameter-Efficient Transformers Serving'을 발표하였다.




# 기타 - KV cache
transformer model의 추론에서 샘플링 과정은 먼저 주어진 프롬프트 또는 컨텍스트를 처리하는 단계가 이루어지며, 그런 다음 추가 토큰을 하나씩 샘플링하면서 (= autoregressive) 트랜스포머는 self-attention을 수행한다. 이 self-attention을 위해 현재 시퀀스에 있는 각 항목의 key-value 값을 필요로 하며, 이러한 벡터들은 KV Cache 또는 Past Cache라고 불리는 행렬로 제공된다. Past Cache는 다음과 같은 형태를 가진다: [batch_size, 2, num_heads, seq_len, features].

토큰을 샘플링할 때마다 이러한 벡터들에 대한 중복계산을 피하는 것이 이것의 목적이다. 이미 계산된 k, v 값이 있다면, 우리는 약간의 저장 비용으로 상당한 양의 계산 비용을 절약할 수 있다.

토큰마다 우리가 새롭게 저장하는 bytes는 2 X 2 X n_layers X n_heads X d_head이다. 가장 앞의 팩터 2는 k, v의 두 벡터의 수를 의미하고, 각 레이어마다의 k, v값을 저장한다. 이 때 k, v 행렬은 n_heads X d_head 행렬이고, 2번째 팩터 2는 bytes의 수를 의미한다. (16-bit 포맷이라고 가정)

이 때 절약할 수 있는 flops는 2 X 2 X n_layer X d_model^2이다. token embedding t_e를 W_k로 곱해야 되며, 여기에 2 X d_model^2 flops가 소요된다. 우리는 k와 v에 대해서 한번씩 계산해야 하므로 2를 곱해주고, 이러한 과정을 n_layer만큼 반복해야 한다.

OPT-30B 모델이 있고, seq_len = 1024, batch_size = 128일 때, KV cache에 소요되는 메모리는 다음과 같다:

`batch_size(128) X 2 X 48(n_layers) X 7168 (d_model) X 1024(seq_len) = 180 GB`


```python

```
