{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2202f03-7ff2-433a-8368-2ba6f3180187",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Few-Shot Question Answering by Pretraining Span Selection(Splinter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2a47d-cb1b-4851-8be1-a4300ef7b484",
   "metadata": {},
   "source": [
    " We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering.  \n",
    " We propose a new pretraining scheme tailored for question answering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span.  \n",
    " \n",
    " 본 논문의 모델인 Splinter(**span-level pointer**)는 question answering에서 새로운 학습 데이터 처리인 **recurring span selection** 을 시도했다. 이 방법은 지문에서 반복되는 정답의 위치를 모두 마스킹 하는 방법이다.\n",
    " \n",
    " <img src=\"https://github.com/indexxlim/indexxlim.github.io/blob/main/diary.py/machine_learning/paper/images/splinter/1_base_size_on_SQuAD.png?raw=true\" itemprop=\"image\" width=\"30%\">\n",
    "few-shot question answering by sampling small training sets from existing question answering benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa0956-286e-4fee-b8cf-6766c85ec91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33339920-eb52-4ad8-8226-9e0744ab6878",
   "metadata": {},
   "source": [
    "## Pre-Training\n",
    "- Tensorflow\n",
    "- Adam Optimizer\n",
    "- 2.4M training steps with batches of 256 sequence of length 512\n",
    "- After warming up, max learning rate 10e-4, after which it decays linearly\n",
    "- 0.1 dropout rate\n",
    "\n",
    "## Fine-Tuning\n",
    "- Hyperparameters from the default conﬁguration of the HuggingFace Transformers package\n",
    "- Adam Optimizer\n",
    "- Fnetuning on 1024 examples or less, train for either 10 epochs or 200 steps\n",
    "- For full-size datasets, train for 2 epochs\n",
    "- Set the batch size to 12 and use a maximal learning rate of 3 * 10−5, which warms up in the ﬁrst 10% of the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9f731-4575-4148-ab4b-d8947c4cc763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9496ade0-2a58-4adf-a768-bdf91aaa2db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
