---
layout: single
title: "Training_of_transformer"
tags: [Attention, Transformer]
use_math: true
comments: true
author: indexxlim
classes: wide

---
# 트랜스포머의 학습!

transformer의 발표 이후로 많은 종류의 논문들이 발표되고, 그에 따른 학습 방법에도 여러종류가 있습니다.
이번 포스트에서는 huggingface의 transformers의 소스코드에 따라 몇개 방법론들의 학습을 진행해보려고 합니다.
그 중 albert, disti
roberta는 GPT-2의 방

## Reference
